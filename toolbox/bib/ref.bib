
@article{limTemporalFusionTransformers2021,
  title = {Temporal {{Fusion Transformers}} for Interpretable Multi-Horizon Time Series Forecasting},
  author = {Lim, Bryan and Ar{\i}k, Sercan {\"O}. and Loeff, Nicolas and Pfister, Tomas},
  year = {2021},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {4},
  pages = {1748--1764},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2021.03.012},
  abstract = {Multi-horizon forecasting often contains a complex mix of inputs \textendash{} including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past \textendash{} without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically `black-box' models that do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) \textendash{} a novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and highlight three practical interpretability use cases of TFT.},
  langid = {english},
  file = {/Users/tinghsi/Zotero/storage/FEDJ4IX7/Lim et al. - 2021 - Temporal Fusion Transformers for interpretable mul.pdf}
}

@article{wuDeepTransformerModels2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08317},
  eprint = {2001.08317},
  eprinttype = {arxiv},
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging selfattention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenzalike illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the stateof-the-art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tinghsi/Zotero/storage/NDUWG6BG/Wu et al. - 2020 - Deep Transformer Models for Time Series Forecastin.pdf}
}


