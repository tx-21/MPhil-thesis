\chapter{Methods and Materials}
\section{Wastewater treatment plant description}
\subsection{Process and data sources in SWHEPP}
Shek Wu Hui Effluent Polish Plant (SWHEPP) is a secondary sewage treatment plant, which treats the municipal wastewater of the Sheung Shui, Fanling Districts and adjacent areas, and treated leachate effluent from North East New Territories (NENT) leachate treatment plant. The plant is designed for 300,000 population equivalents (PE) in 2001, and in 2009, the daily treatment capacity has been expanded from 80,000 m$^3$/day to 93,000 m$^3$/day. SHWEPP is operated and maintained by Drainage Services Department (DSD), and the plant will be upgraded to teritary treatment level to increase the treatment capacity of 190,000 m$^3$/day by the end of 2025. As shown in Fig.~\ref{fig:SHWEPP-flowchart}, the treatment plant is mainly comprised of primary sedimentation, secondary biological treatment, and final sedimentation followed by a membrane bioreactor (MBR), which provides an advanced level of organic and suspended solids removal. To monitor the effluent quality in real-time, low volume of the MBR effluent is pumped to an effluent container near by the MBR location. Two on-line meters, ammoniacal nitrogen on-line sensor and colour level on-line analyzer are installed in the effluent container, which are indicated as (a) and (b) in Fig.~\ref{fig:SHWEPP-flowchart}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/Sewage-treatment-process-flowchart.png}
    \caption{Sewage treatment process flowchart at SWHEPP (adapted from Drainage Services Department 2020)}
    \label{fig:SHWEPP-flowchart}
\end{figure}

\subsection{Reclaimed water standard}
Reclaimed water for non-potable reuses can serve for irrigation for agricultures, toilet flushing and irrigation for landscaping, etc. Water Supply Department (WSD) will soon implement a reclaimed water supply system in SWHEPP by disinfecting the teriary treated sewage (i.e., MBR permeate). The produced reclaimed water will be served for non-potable reuses and is required to satisfy the water quality standards, shown in Table.~\ref{tab:reclaimed-standard}.

\begin{table}[!ht]
    \centering
    \caption{\label{tab:reclaimed-standard}Endorsed Reclaimed Water Quality Standards from Water Supply Department.}
    \begin{NiceTabular}{lcl}
        \toprule
        Parameter & Unit & Requirement \tabularnote{The water quality standards for all parameters are applicable at the point-of-use of the system.} \\
        \midrule
        \textit{E. coli} & cfu/100 mL & Not detectable \\ 
        Colour & Hazen Unit & $\le$ 20 \\ 
        Ammoniacal Nitrogen (NH$_3$-N) & mg/L as N & $\le$ 1 \\ 
        Total Residual Chlorine & mg/L & $\ge$ 0.2 \\ 
        Dissolved Oxygen & mg/L & $\ge$ 0.2 \\ 
        Turbidity & NTU & $\le$ 5 \\ 
        5-day Biochemical Oxygen Demand & mg/L & $\le$ 1 \\ 
        pH & - & 6-9 \\ 
        Threshold Odour Number & - & $\le$ 100 \\ 
        Synthetic detergents & mg/L & $\le$ 5 \\
        \bottomrule
    \end{NiceTabular}
\end{table}

\section{Data collection and preparation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/instrument/sampling-tank.png}
    \caption{Colour levels and ammonia concentration are measure in the effluent container (i.e., on the right of the image.) A water pump transports MBR effluent to the effluent container continuously at real-time. The black vault on the left of the image contains a laptop and a colour spectrophotometer.} 
    \label{fig:sampling-tank}
 \end{figure}

\subsection{On-line data monitoring and collection}
To enable us to perform on-line monitoring of ammonium concentration (NH$_{3}$-N) in the MBR effluent, a Ammonium and Potassium Probe, AmmoLyt$^\circledR$Plus 700 IQ (Xylem Company) is installed as in Fig.~\ref{fig:nh3-sensor-a} in the effluent container, as shown in Fig.~\ref{fig:sampling-tank}. The operation was commenced on 27 April 2021 and completed on 27 March 2022. The ion-selective electrode (ISE) probe provides continuous and reagentless monitoring of ammonium and potassium at the configured interval of one measurement per minute. Due to the ISE probe cannot differentiate the potentials difference cause by ammonium and potassium ions in the electrodes, the on-line monitoring of ammonium concentration requires the continuous calibration using potassium concentration.

The instrument records ammonium concentration as NH$_{4}$-N mg/L, a form to express the sum of nitrogen found in reduced nitrogen (III) form. Ammonia has a reported pKa of 9.25 \citep{nationalcenterforbiotechnologyinformationPubChemCompoundSummary2022}, meaning ammonium is a primary species under the pH of 9.25 in water. In WWTPs, the pH in water normally ranges from pH of 7--8, making the NH$_{4}$-N concentration the dominant species. Both ammonia and ammonium contain one nitrogen atom, 1 mg/L NH$_{3}$-N is the same as 1 mg/L NH$_{4}$-N. Thus, to prevent confusion, in the following paragraph the unit of NH$_{4}$-N will be expressed by NH$_{3}$-N, which is the unit used in the water quality standard. The collection of on-line ammonia data is achieved through downloading csv files from the website connected to the IQ Sensor Controller (Xylem Comapny), as shown in Fig.~\ref{fig:nh3-sensor-b}. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/instrument/ammonium-sensor.png}
      \caption{AmmoLyt$^\circledR$Plus 700 IQ, Xylem} \label{fig:nh3-sensor-a}
    \end{subfigure}%
    \hspace{5em}%   % maximize separation between the subfigures
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/instrument/controller-for-iq-sensor.png}
      \caption{DIQ/S 284-EF controller, Xylem} \label{fig:nh3-sensor-b}
    \end{subfigure}%  
  \caption{Instrument of on-line ammonium monitoring system.} \label{fig:nh3-sensor}
\end{figure}

An hourly monitoring of the colour levels of MBR effluent was conducted from 5 October 2021 to 26 February 2022 by using a custom-made on-line colour analysis system. Originally, the spectrophotometer as Fig.~\ref{fig:hach-sip} and a peristaltic pump as Fig.~\ref{fig:hach-dr3900} can only initiate a single measurement of colour level by pressing the "READ" buttom on the DR3900 panel. To realize continuously sampling and analyzing colour level without human intervention, an actuator with programmable time function was mounted on the panel of DR3900, as shown in Fig.~\ref{fig:hach-actuator}. 

The automatic sampling and analzying of the colour level begins with the action of the actuator, by clicking on the "READ" button to initiate the colour analysis at a fixed interval of 30 minutes. 3 mL of sample was collected from the effluent container and delivered to the spectrophotometer cell. Then, the sample was subsequentially analysed by the spectrophotometer with the data transmitted to an automatic data acquisition and storage software pre-installed in the laptop. The DR3900 device is connected to a laptop, which receives the real-time data and stores on a data management software from Hach company. To access the real-time data from the laptop, Google Remote Desktop is used to operate the laptop via Internet cloud services using any devices having access to the Internet. The entire process is illustrated in Fig.~\ref{fig:diagram-colour-analysis}. After the measurement, the sample will be discharged to the effluent container and the online colour monitoring system is left idle until the next measurement. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/instrument/SIP10.png}
      \caption{SIP10 peristaltic pump, Hach} \label{fig:hach-sip}
    \end{subfigure}%
    \hspace{2em}   % maximize separation between the subfigures
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/instrument/DR3900_PS.png}
      \caption{DR3900 spectrophotometer, Hach} \label{fig:hach-dr3900}
    \end{subfigure}%
    \vspace{2em}
    \begin{subfigure}{0.7\textwidth}
        \includegraphics[width=\linewidth]{imgs/instrument/actuator-mount.png}
        \caption{Customized clicker/actuator} \label{fig:hach-actuator}
    \end{subfigure}%  
  \caption{Instruments of on-line colour analysis system.} \label{fig:hach}
\end{figure}

The maintenance and calibration of the DR3900 spectrophotometer is performed on a weekly basis. During the maintenance, the DR3900 device was shut off, and chlorine solution at the concentration of 100 mg/L was pumped into the sampling tubes and the plastic cuvette for disinfection and cleansing. The cleanse of the tubes and cuvette were manually inspected with eyes to make sure no foreign objects were stuck inside. De-ionized water was brought to the site to perform the spectrophotometer calibration after the reboot of DR3900. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/instrument/colour-sampler.png}
    \caption{Schematic diagram of the custom-made on-line colour anlaysis system.}
    \label{fig:diagram-colour-analysis}
 \end{figure}

\subsection{Loss function for model evaluation}
Loss functions are used to determine the error between the model outputs (i.e., prediction or forecasting values) and the given target value \citep{deepaiLossFunction2022}. The bigger the difference between the ground truth \boldmath${y}$ and the model outputs \boldmath${\hat{y}}$, the higher the value of the loss function is, meaning the model perfomred poorer. A low value for the loss means the modle perfomred well. The selection of the types of the loss function is essential for training the model to perform specific tasks. In this study, Mean Squared Error (MSE) is used for evaluating the regression models. The values of MSE will never be negative, and is formally defined by the following equation:

\begin{equation}\label{eq-mse}
    MSE=\frac{\sum (y_i-\hat{y_i})^2}{n}
\end{equation}

\subsection{Data cleaning and pre-processing}
In this study, ammonia concentration and colour level forecasting models will be trained, and the model training steps are shown in Fig.~\ref{fig:training-scheme}. The training processes are split into two sections, one is the baseline model training steps, the other is proposed model training steps. The training steps of the first section used cleaned data to train forecasting models and generated baseline model performance, which will be further compared with the model perfromance generated from the second section. The second section includes using pre-processed datasets (i.e., data smoothing) and feature engineering enhanced datasets to train the forecasting model. In machine learning, the data used for training models is refered to model inputs, features and variables.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/pre-processing/training-scheme.png}
    \caption{Machine learning model training steps.}
    \label{fig:training-scheme}
\end{figure}

The raw data embedded in the original csv files exists many issues, such as missing values, having extreme low or high values, and unreadable texts, etc. Thus, the data cleaning and pre-processing are necessary for more effective process of model training. Python programming language and related modules of Numpy and Pandas were used to clean and pre-process the raw dataset for further usage. The ammonia raw dataset contained 44,640 samples (data points) with 8 variables, giving a matrix size of 44,640 x 8, and the samples were collected in time series at 1 minute interval. The colour level raw dataset contained 1488 samples with 34 variables, giving a matrix size of 1488 x 34, and the samples were collected in time series at 30 minute interval.

Before the high-resolution data from colour and ammonia datasets were compressed into time series data at 1 hour interval via averaging, extreme values were manually removed. For ammonia dataset, we replaced the values higher than 7.0 mg/L with NaN (i.e., Not a number), and futher use interpolation to fill up the NaN along with the missing values in the dataset. For colour dataset, we manually took out the relatively low data points on the days when the maintenance and calibration tasks were performed; extremely values higher than 300 Hazen Unit were also replaced by NaN. Same as the data cleaning method used for ammonia dataset, the missing values and NaN were filled up via interpolation.

\subsubsection{Data smoothing with Savitzky-Golay and EWMA filter}
Data smoothing was performed on both ammonia and colour datasets using the same method. One of the effective ways to remove the noise from the dataset is to apply data smoothing filters. Two filteres were applied in this study, Savitzky-Golay (SG) and Exponentially Weighted Moving Average (EWMA) filters.

A SG filter is a digital filter that can be applied to a set of digital data points for the purpose of smoothing the data without distorting the data tendency. This is achieved via convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares \citep{wikipediaSavitzkyGolayFilter2022}. The illustration is shown in Fig.~\ref{fig:filters-sg} and the procedures of how data points are smoothed is presented in the following steps:

\noindent
\begin{myenumerate}
    \item Extract short-time window (i.e., blue dots in Fig.\ref{fig:filters-sg})
    \item Determine polynomial degree (e.g., different polynomial degree is compared in Fig.~\ref{fig:filters-sg}).
    \item Find the smoothed data point (i.e., at center of the window).
    \item Repeat for shifted window (e.g., similar to moving average).
\end{myenumerate}

The equation to decribed the smoothed value of \boldmath$Y_j$ can be expressed in Eq.~\ref{sg-eq}:

\begin{equation}\label{sg-eq}
    Y_j=(C\otimes y)_j=\sum_{i=\frac{1-m}{2}}^{\frac{m-1}{2}}C_iy_{j+i},\,\frac{m+1}{2}\le j\le n-\frac{m-1}{2}
\end{equation}

\noindent
where $Y_j$ corresponds to the $j^{th}$ smoothed data point, $m$ to the window size (i.e., numer of data points intended to smooth out) and $C_i$ to the convolution coefficients (i.e., determined by \citet{savitzkySmoothingDifferentiationData1964}). 

Exponentially weighted moving average (EWMA), also known as auto-regressive (AR) filtering, is a technique that filters measurements. An EWMA filter smoothes a measured data point by exponentially averaging that particular point with all previous measurements. The EWMA equation can be expressed in Eq.~\ref{ewma-eq}:

\begin{equation}\label{ewma-eq}
    \begin{aligned}
        &\alpha=\frac{2}{span+1} \\
        &y_0=x_0 \\
        &y_t=(1-\alpha)y_{t-1}+\alpha x_t
    \end{aligned}
\end{equation}

\noindent
where $\alpha$ corresponds to the decay paratmeter, $x_t$ to the value at a time period, $y_t$ to the value of the EWMA at any time period t, span to the window size.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
      \includegraphics[width=\linewidth]{imgs/pre-processing/demo-polynomial-fitting.png}
      \caption{SG filter with different polynomial degree \citep{taalSmoothingYourData2017}.} \label{fig:filters-sg}
    \end{subfigure}%
    \hspace{2em}%   % maximize separation between the subfigures
    \begin{subfigure}[t]{0.5\textwidth}
      \includegraphics[width=\linewidth]{imgs/pre-processing/demo-weight-ewma.png}
      \caption{Examples of weights with exponential decay at varied alpha values \citep{cfiExponentiallyWeightedMoving2022}.} \label{fig:filters-ewma}
    \end{subfigure}%  
  \caption{Illustration of the influence of different polynomial degrees in the fitting of SG filter and the weigth decay with varied alpha values in EWMA filter.} \label{fig:filters}
\end{figure}

Both SG and EWMA filters are required to select the hyperparamters, the selected values are presented in Table.~\ref{tab:filter-hyperparameters}.

\begin{table}[!ht]
    \centering
    \caption{The selected hyperparameters for SG and EWMA filters.}\label{tab:filter-hyperparameters}
    \begin{NiceTabular}{lcc}
        \toprule
        Group Name & Window size & Polynomial degree \\
        \midrule
        SG-5   & 5 & 2 \\ 
        SG-7   & 7 & 2 \\ 
        SG-9   & 9 & 2 \\ 
        EWMA-2 & 2 & - \\ 
        EWMA-3 & 3 & - \\ 
        EWMA-4 & 4 & - \\ 
        \bottomrule
    \end{NiceTabular}
\end{table}

\subsubsection{Outlier Removal}
Despite the extreme values in the ammonia raw dataset were removed based on simple conditions (i.e., concentration higher than 7.0 mg/L), the ammonia sensor can still capture unideal data points collectively. In the outlier removal process, we intended to identify the collective faults of ammonia data in the unit of an entire day. To determine whether the ammonia data on a specific day shows collective fault, two abnormal conditions are defined:

\noindent
\begin{myenumerate}
    \item NH$_{3}$-N fluctuation $\le$ 0.1 (i.e., lower than the sensor resolution).
    \item No diurnal fluctuation (i.e., Fluctuation = peak value – bottom line value).
\end{myenumerate}

To automiatically realize the identification of noraml or abnormal signals, peak analysis was performed on the daily ammonia data. The analysis takes a one-dimension array (i.e., the data form of ammonia in a day) and finds all local maximum values by simple comparison of neighboring values. This function will also provide information such as width and prominence, as in Fig.~\ref{fig:prominence} to help us identify whether the diurnal fluctuation is existed.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\columnwidth]{imgs/pre-processing/prominence.png}
    \caption{Illustration of peak analysis. Four important elements are automatic caculated by the function \citep{mathworksDocumentationFindpeaks2022}.}
    \label{fig:prominence}
 \end{figure}

\subsubsection{Feature Engineering}
To create addition features from the raw datasets, we have carefully observed and analyzed the SWHEPP influent. We discovered that the SWHEPP influent is consisted of treated landfill effluent from NENT landfill leachate site and municipal wastewater, as shown in Fig.~\ref{fig:geomap}. Treated leachate effluent mainly contributes the colour levels in the SHWEPP influent, while the ammonia concentration is mostly from the municipal wastewater. During the mixing of both type of the wastewater as in Fig.~\ref{fig:blend-ratio}, pollutants contribute to colour levels will be diluted by the municipal wastewater, same as the opposite for the dilution of ammonia concentration. The changes of colour levels and ammonia concentration are related, thus, in feature engineering, colour level data was selected for training ammonia forecasting model; ammonia data was selected for training colour forecasting model, as shown in Fig.~\ref{fig:feature-selection}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/pre-processing/geomap.png}
    \caption{Sewer system coverage of SHWEPP. The covered areas (i.e., area circled in red boundary) include Fanling/Sheung-Shui new towns and NENT landfill leachate treament plant.}
    \label{fig:geomap}
 \end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\linewidth]{imgs/pre-processing/blending-ratio.png}
      \caption{Flowchart showing the blending of treated leachate effluent with municipal wastewater.} \label{fig:blend-ratio}
    \end{subfigure}%
    \hspace{2em}%   % maximize separation between the subfigures
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\linewidth]{imgs/pre-processing/pos-encoding.png}
      \caption{Positional encoding of hour components.} \label{fig:pos-encoding}
    \end{subfigure}%  
  \caption{Analysis of influent quality composition and the illustration of the positional encoding.} \label{fig:blend-pos}
\end{figure}

The new features are inspired from the research work of \citet{abu-bakarQuantifyingImpactCOVID192021}. The author pointed out the four types of hourly household water consumption patterns as in Fig.~\ref{fig:water-consumption-pattern}, which correlates the specific time of the day to the volume of the water consumed in households. In other words, as fresh water is consumed, wastewater is generated at the same time, the wastewater then enters the public sewage system and result in the increase of ammonia concentration. Thus, it is convinced that time features will be able to help the machine learning models to better correlate and predict the change of ammonia concentration in the wastewater. 

Time feature is realized through a technique called positional encoding (POS). The positioanl encoded feature was achieved as the following steps:

\noindent
\begin{myenumerate}
    \item The timestamp are represented as three elements---hour, day and month.
    \item Each element will bed decomposed into sine and cosine components.
    \item Last step is applied to hours and days to make all elements represented cyclically.
\end{myenumerate}

Due to the size of the datasets used in this study for training ammonia and colour forecasting model is 31 days, only hour element was transformed into sine and cosine components as in Fig.~\ref{fig:pos-encoding}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/pre-processing/hourly-consumption-pattern.png}
    \caption{Illustration of feature selections for model training.}
    \label{fig:water-consumption-pattern}
 \end{figure}

\subsection{Data transformation}
Before the pre-processed data is fed into the models for training, we need to split the data into three clusters, which are training (60\%), validation (20\%), and testing dataset (20\%). A taining dataset is a set of examples (e.g., historical data) for models to learn the hidden trends and information in the data, shown in (a) in Fig.~\ref{fig:training-scheme}, and the training loss is calculated by taking the sum of loss for each example in the training dataset after each epoch. Since it is impossible to have the optimized hyperparameters in the first try of the training, a validation dataset as in (b) in Fig.~\ref{fig:training-scheme} is used to assess the model performance until we obtain the optimzed settings. The validation loss plays an important role during the model training, the adjustments of the hyperparameters will directly reflect on the change of the validation loss, the lower the values, the better the model performance is. As the optimzed model is obtained, testing dataset is used to evaluate the performance of the forecasting model, as shown in (c) in Fig.~\ref{fig:training-scheme}. To the forecasting Models, testing dataset has never been seen by the models. If the model tuning process was performed on the testing dataset, the model performance would be a biased result since the hyperparameters are revised in favor to the evalution of the testing dataset.

In Fig.~\ref{fig:training-scheme}, the hyperparameters will remained the same once the optimzed values are found, thus generating a baseline model perfromance of a specific machine learning model. The baseline results will be further compared with the results from the model trained by the proposed model trianing steps, which include datasets that have been performed data smoothing and feature engineering techniques.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/pre-processing/feature-selection.png}
    \caption{Illustration of feature selections for model training.}
    \label{fig:feature-selection}
 \end{figure}

%\section{Architecture design of selected baseline models}
\subsection{The use of software and machine learning framework}
To efficiently build machine learning models, PyTorch framework was used in this study. 

PyTorch is an open source machine learning framework based on the Torch library,[3][4][5] used for applications such as computer vision and natural language processing,[6] primarily developed by Meta AI.[7][8][9] It is free and open-source software released under the Modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[10]

A number of pieces of deep learning software are built on top of PyTorch, including Tesla Autopilot,[11] Uber's Pyro,[12] Hugging Face's Transformers,[13] PyTorch Lightning,[14][15] and Catalyst.[16][17]

PyTorch provides two high-level features:[18]

Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)
Deep neural networks built on a tape-based automatic differentiation system


\subsection{Random Forest}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\linewidth]{imgs/models/random-forest.png}
      \caption{Random Forest (RF) \citep{riebesellRandomForest2022}.} \label{fig:rf}
    \end{subfigure}%
    \hspace{3em}%   % maximize separation between the subfigures
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/models/DNN.png}
        \caption{Deep Neural Network (DNN) \citep{ibmNeuralNetworks2022}.} \label{fig:dnn}
    \end{subfigure}%
  \caption{Random } \label{fig:rf-dnn}
\end{figure}

F can be described as an ensemble method in which the final result is obtained by aggregating (through averaging in the case of regression) results from multiple weak learners known as Classification and Regression Trees (CARTs) (Breiman, 2017). Each weak learner (tree) is trained on the bootstrap set, which is obtained by sampling with replacement from the original training set. For trees, the input variables are used to generate nodes. These variables are selected partially and randomly as a subset in every split, then the variable contributing to the smallest sum of impurity of two child nodes at a certain split point is chosen as the split variable. This is done repeatedly until the trees don't need to split anymore. The regression impurity of a particular node is defined by Eqs. (2), (3) and (4), \citep{wangMachineLearningFramework2021}

\subsection{Deep Neural Networks}

\subsection{Recurrent Neural Network}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/models/rnn-2.png}
      \caption{Recurrent Neural Network (RNN).} \label{fig:rnn}
    \end{subfigure}%
    \hspace{1em}%   % maximize separation between the subfigures
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\linewidth]{imgs/models/gru-2.png}
        \caption{Gate Recurrent Unit (GRU).} \label{fig:gru}
    \end{subfigure}%
    \hspace{1em}
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/models/lstm-2.png}
      \caption{Long Short-term Memory (LSTM).} \label{fig:lstm}
    \end{subfigure}%  
  \caption{Different recurrent neural networks (adapted from \citet{olahUnderstandingLSTMNetworks2015})..} \label{fig:recurrent-nn}
\end{figure}


\subsection{Gate Recurrent Unit}

\subsection{Long Short-term Memory}
Recently, deep recurrent neural networks (RNN) such as long short-term memory networks (LSTM) have shown breakthrough results over state-of-the-art machinelearning methods in many applications with non-linear temporal data, including robotics, high-energy physics and computational geometry (Goodfellow et al. 2016). These methods can successfully engineer appropriate long-term temporal dependencies and variable length features, significantly lessening the need to pre-process data with respect to traditional machine-learning methods or statistical approaches. It is the ability to capture the long-term dependencies that make LSTM networks particularity fitting for the problem at hand. 

Fig. 1 The general schema of a RNN unit versus a LSTM one (adapted from Olah 2015)

Architecture of the method proposed by Mamandipoor et al. (2020)

%\section{Implementation of regularization}
%\subsection{Scheduler}

